{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GAlJ0x4xn6o",
        "outputId": "e4de8ab8-0671-46ba-a727-df4de410eb8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.10/dist-packages (2024.1.2)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.10/dist-packages (from pycuda) (2024.1.14)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (4.3.6)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pycuda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda.autoinit\n",
        "import pycuda.driver as drv\n",
        "from pycuda.compiler import SourceModule\n",
        "import numpy as np\n",
        "\n",
        "# Convolution Kernel\n",
        "conv_mod = SourceModule(\"\"\"\n",
        "__global__ void conv2d(float *input, float *kernel, float *output, int H, int W, int KH, int KW, int stride) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    float sum = 0;\n",
        "    if(row * stride < H && col * stride < W) {\n",
        "        for (int i = 0; i < KH; ++i) {\n",
        "            for (int j = 0; j < KW; ++j) {\n",
        "                sum += input[(row * stride + i) * W + (col * stride + j)] * kernel[i * KW + j];\n",
        "            }\n",
        "        }\n",
        "        output[row * W + col] = sum;\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "conv2d_kernel = conv_mod.get_function(\"conv2d\")\n",
        "\n",
        "def pad_input(input, pad_size):\n",
        "    H, W = input.shape\n",
        "    padded_input = np.zeros((H + 2 * pad_size, W + 2 * pad_size), dtype=np.float32)\n",
        "    padded_input[pad_size:H + pad_size, pad_size:W + pad_size] = input\n",
        "    return padded_input\n",
        "\n",
        "def conv2d_layer(input, kernel, stride=1, padding=1):\n",
        "    input = pad_input(input, padding)\n",
        "    H, W = input.shape\n",
        "    KH, KW = kernel.shape\n",
        "    output_H = (H - KH) // stride + 1\n",
        "    output_W = (W - KW) // stride + 1\n",
        "    output = np.zeros((output_H, output_W), dtype=np.float32)\n",
        "\n",
        "    block_size = (16, 16, 1)\n",
        "    grid_size = (output_W // block_size[0] + 1, output_H // block_size[1] + 1, 1)\n",
        "\n",
        "    conv2d_kernel(drv.In(input), drv.In(kernel), drv.Out(output), np.int32(H), np.int32(W), np.int32(KH), np.int32(KW), np.int32(stride), block=block_size, grid=grid_size)\n",
        "    return output\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def flatten(x):\n",
        "    return x.flatten()\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Define CNN Layers\n",
        "conv_kernel1 = np.random.randn(3, 3).astype(np.float32)\n",
        "conv_kernel2 = np.random.randn(3, 3).astype(np.float32)\n",
        "dense1_weights = np.random.randn(25, 10).astype(np.float32)  # 3x3 convolutional output flattened\n",
        "dense1_bias = np.random.randn(10).astype(np.float32)\n",
        "output_weights = np.random.randn(10, 3).astype(np.float32)  # 3 output neurons for U, G, N\n",
        "output_bias = np.random.randn(3).astype(np.float32)\n",
        "\n",
        "def forward_pass_cnn(input):\n",
        "    conv1 = relu(conv2d_layer(input, conv_kernel1, stride=1))\n",
        "    conv2 = relu(conv2d_layer(conv1, conv_kernel2, stride=1))\n",
        "    flat = flatten(conv2)\n",
        "    dense1_output = relu(np.dot(flat, dense1_weights) + dense1_bias)\n",
        "    output = sigmoid(np.dot(dense1_output, output_weights) + output_bias)\n",
        "    return conv1, conv2, dense1_output, output\n",
        "\n",
        "def backpropagation_cnn(input, conv1, conv2, dense1_output, output, target, learning_rate):\n",
        "    global output_weights, output_bias, dense1_weights, dense1_bias, conv_kernel1, conv_kernel2\n",
        "\n",
        "    # Reshape output_delta to match output_bias shape\n",
        "    output_error = output - target.flatten()  # Flatten target to (3,)\n",
        "    output_delta = output_error * sigmoid_derivative(output)\n",
        "\n",
        "    dense1_error = output_delta.dot(output_weights.T)\n",
        "    dense1_delta = dense1_error * relu_derivative(dense1_output)\n",
        "\n",
        "    conv2_error = dense1_delta.dot(dense1_weights.T).reshape(conv2.shape) * relu_derivative(conv2)\n",
        "\n",
        "    output_weights -= learning_rate * np.outer(dense1_output, output_delta)\n",
        "    output_bias -= learning_rate * output_delta  # Now shapes should match\n",
        "    dense1_weights -= learning_rate * np.outer(flatten(conv2), dense1_delta)\n",
        "    dense1_bias -= learning_rate * dense1_delta\n",
        "    conv_kernel2 -= learning_rate * conv2_error\n",
        "    conv_kernel1 -= learning_rate * conv2_error\n",
        "\n",
        "# Training data\n",
        "inputs = {\n",
        "    \"U\": [\n",
        "        (np.array([[1,0,0,0,1], [1,0,0,0,1], [1,0,0,0,1], [1,0,0,0,1], [1,1,1,1,1]]).astype(np.float32).flatten(), np.array([1, 0, 0]))\n",
        "    ],\n",
        "    \"G\": [\n",
        "        (np.array([[1,1,1,1,1], [1,0,0,0,0], [1,0,0,1,1], [1,0,0,0,1], [1,1,1,1,1]]).astype(np.float32).flatten(), np.array([0, 1, 0]))\n",
        "    ],\n",
        "    \"N\": [\n",
        "        (np.array([[1,0,0,0,1], [1,1,0,0,1], [1,0,1,0,1], [1,0,0,1,1], [1,0,0,0,1]]).astype(np.float32).flatten(), np.array([0, 0, 1]))\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZn2eesdztdy",
        "outputId": "b9b8265e-5bd8-44ac-9b4a-e260fc0a3076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: module in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the CNN\n",
        "epochs = 1000\n",
        "learning_rate = 0.01\n",
        "for epoch in range(epochs):\n",
        "    for key, data_list in inputs.items():\n",
        "        for input_data, target in data_list:\n",
        "            input_data = input_data.reshape(5, 5).astype(np.float32)\n",
        "            target = target.reshape(1, -1)\n",
        "            conv1, conv2, dense1_output, output = forward_pass_cnn(input_data)\n",
        "            backpropagation_cnn(input_data, conv1, conv2, dense1_output, output, target, learning_rate)\n",
        "\n",
        "# Testing data\n",
        "testing_inputs = {\n",
        "    \"U\": [\n",
        "        (np.array([[1,0,0,0,1], [1,0,0,0,1], [1,0,0,0,1], [1,0,0,0,1], [0,1,1,1,0]]).astype(np.float32), \"U\")\n",
        "    ],\n",
        "    \"G\": [\n",
        "        (np.array([[1,1,1,1,1], [1,0,0,0,0], [1,0,1,1,1], [1,0,0,0,1], [1,1,1,1,1]]).astype(np.float32), \"G\"),\n",
        "        (np.array([[1,1,1,1,1], [1,0,0,0,0], [1,0,1,1,1], [1,0,1,0,1], [1,1,1,1,1]]).astype(np.float32), \"G\")\n",
        "    ],\n",
        "    \"N\": [\n",
        "        (np.array([[1,1,0,0,1], [1,1,0,0,1], [1,0,1,0,1], [1,0,0,1,1], [1,0,0,1,1]]).astype(np.float32), \"N\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Testing the CNN\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "for key, data_list in testing_inputs.items():\n",
        "    print(f\"Class: {key}\")\n",
        "    for input_data, true_label in data_list:\n",
        "        input_data = input_data.reshape(5, 5).astype(np.float32)\n",
        "        _, _, _, output = forward_pass_cnn(input_data)\n",
        "        predicted_label = [\"U\", \"G\", \"N\"][np.argmax(output)]\n",
        "        print(f\"Predicted: {predicted_label}, True: {true_label}\")\n",
        "        if predicted_label == true_label:\n",
        "            correct_predictions += 1\n",
        "        total_predictions += 1\n",
        "\n",
        "accuracy = (correct_predictions / total_predictions) * 100\n",
        "print(f\"Accuracy: {accuracy}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "0KKz_YeItovm",
        "outputId": "b4d8fafb-bf04-4dfb-e393-1ac78388210c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (3,3) (5,5) (3,3) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-c5062562ffcd>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense1_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mbackpropagation_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense1_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-4f1492dd1296>\u001b[0m in \u001b[0;36mbackpropagation_cnn\u001b[0;34m(input, conv1, conv2, dense1_output, output, target, learning_rate)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mdense1_weights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense1_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mdense1_bias\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdense1_delta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mconv_kernel2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconv2_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mconv_kernel1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconv2_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,3) (5,5) (3,3) "
          ]
        }
      ]
    }
  ]
}